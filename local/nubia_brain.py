import os
import pickle
import torch
import gspread
import time
import uuid
from typing import Optional, Tuple, Any
from google.oauth2.service_account import Credentials
from sentence_transformers import SentenceTransformer, util
from config import NUBIA_CREDENTIALS, API_OPENAI
from datetime import datetime
from gtts import gTTS
from openai import OpenAI
import re


CLASSIFY_MODEL = "gpt-4o-mini"
PRIVACY_MODEL  = "gpt-4o-mini"
VERIFY_MODEL   = "gpt-4o-mini"


HUMANIZE_MODEL = "gpt-4o"
EXPAND_MODEL   = "gpt-4o"

# Vetores/cache
MASTER_SPREADSHEET_NAME = "NUBIA"
CACHE_VETORES = "cache_vetores.pkl"


client = OpenAI(api_key=API_OPENAI)


modelo_sentenca = None

# ---------------------
# Utils: OPENAI wrapper
# ---------------------
def consultar_openai(model: str,
                     prompt: str,
                     temperature: float = 0.0,
                     max_tokens: int = 1024,
                     system_msg: str = "Voc√™ √© um assistente √∫til.") -> Optional[str]:
    """
    Chama a OpenAI com sistema de RETRY autom√°tico.
    """
    max_retries = 3
    delay_base = 5

    for tentativa in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content.strip()

        except Exception as e:
            erro_str = str(e)
            print(f"‚ö†Ô∏è Erro OpenAI (tentativa {tentativa+1}): {e}")
            if "RateLimitError" in erro_str or "429" in erro_str:
                time.sleep(delay_base)
                delay_base *= 2
            else:
                return None
    
    print("‚ùå Falha na OpenAI ap√≥s todas as tentativas.")
    return None

# ---------------------
# Mapa de navega√ß√£o (menu)
# ---------------------
MAPA_NUBIA = {
    "Autoriza√ß√µes M√©dicas (SERAMO)": {
        "tipo": "submenu",
        "opcoes": {
            "Consultas e Exames": "Como solicitar autoriza√ß√£o para consultas e exames m√©dicos?",
            "Tratamentos Seriados (Fisio/Psico/Fono)": "Quais as regras e prazos para tratamentos seriados como fisioterapia e psicologia?",
            "Cirurgias e Interna√ß√µes": "Como solicitar autoriza√ß√£o para cirurgias e interna√ß√µes?",
            "Home Care": "Como funciona e como solicitar o servi√ßo de Home Care?",
            "TFD (Tratamento Fora de Domic√≠lio)": "Como solicitar Tratamento Fora de Domic√≠lio (TFD)?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Odontologia (SERAMO Odonto)": {
        "tipo": "submenu",
        "opcoes": {
            "Autoriza√ß√£o de Tratamento": "Como solicitar autoriza√ß√£o para tratamento odontol√≥gico?",
            "Reembolso Odontol√≥gico": "Como solicitar reembolso de despesas odontol√≥gicas?",
            "Ortodontia (Aparelho)": "Quais as regras e per√≠cias para uso de aparelho ortod√¥ntico?",
            "Per√≠cias Odontol√≥gicas": "Como e onde realizar a per√≠cia odontol√≥gica?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Sa√∫de Ocupacional e Atestados (SERSAO)": {
        "tipo": "submenu",
        "opcoes": {
            "Enviar/Homologar Atestado": "Como fa√ßo para enviar e homologar meu atestado m√©dico?",
            "Prorroga√ß√£o de Afastamento": "Como solicitar prorroga√ß√£o do afastamento m√©dico?",
            "Junta M√©dica": "Quando √© necess√°rio passar por junta m√©dica?",
            "Exames Peri√≥dicos (EPS)": "Como realizar os exames peri√≥dicos de sa√∫de (EPS)?",
            "Teletrabalho e ASO": "Como emitir o ASO para teletrabalho?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Cadastro e Benef√≠cios (SEABE)": {
        "tipo": "submenu",
        "opcoes": {
            "Inclus√£o de Dependentes": "Como fa√ßo para incluir dependentes no Pro-Social?",
            "Carteirinha Digital": "Como obter a carteirinha digital do plano?",
            "Aux√≠lio-Natalidade": "Como solicitar o aux√≠lio-natalidade?",
            "Aux√≠lio-Pr√©-Escolar": "Como solicitar o aux√≠lio pr√©-escolar?",
            "Coparticipa√ß√£o": "Como funciona a coparticipa√ß√£o no Pro-Social?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Financeiro e Reembolsos (SEFAT)": {
        "tipo": "submenu",
        "opcoes": {
            "Reembolso M√©dico/OPME": "Como solicitar reembolso de despesas m√©dicas e OPME?",
            "Glosa e Faturamento": "Como verificar glosas e faturas?",
            "Demonstrativo/Pagamentos": "Como consultar pagamentos e demonstrativos?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Rede Credenciada (SERCRE)": {
        "tipo": "submenu",
        "opcoes": {
            "Consultar Rede": "Como consultar a rede credenciada de m√©dicos e cl√≠nicas?",
            "Credenciamento de Prestador": "Como um m√©dico ou cl√≠nica pode se credenciar ao Pro-Social?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Bem-Estar e Apoio (NUBES/SESAI)": {
        "tipo": "submenu",
        "opcoes": {
            "Programas de Bem-Estar": "Quais s√£o os programas de bem-estar e qualidade de vida do NUBES?",
            "Acolhimento Psicossocial": "Como solicitar apoio psicossocial ou acolhimento?",
            "Casos Complexos de Sa√∫de": "Como a SESAI atua em casos complexos de sa√∫de?",
            "Voltar ao In√≠cio": "MENU_INICIAL"
        }
    },
    "Outros Assuntos / Digitar Livremente": {
        "tipo": "acao_livre",
        "texto": ""
    }
}

def get_mapa_nubia():
    return MAPA_NUBIA

def formatar_texto_menu(chave_menu):
    """
    Gera o texto do menu e o dicion√°rio de op√ß√µes v√°lidas.
    Retorna: (texto, opcoes_validas)
    """
    mapa = get_mapa_nubia()
    texto = ""
    opcoes_validas = {}

    if chave_menu == "MENU_INICIAL":
        texto = "*Ol√°! Sou a NUBIA. Como posso ajudar?* üëá\n\n"
        for i, chave in enumerate(mapa.keys(), 1):
            texto += f"*{i}.* {chave}\n"
            opcoes_validas[str(i)] = chave
        texto += "\n_(Digite o n√∫mero da op√ß√£o)_"
    elif chave_menu in mapa:
        dados = mapa[chave_menu]
        if dados["tipo"] == "submenu":
            texto = f"*{chave_menu}* üëá\n\n"
            for i, (label, pergunta_real) in enumerate(dados["opcoes"].items(), 1):
                texto += f"*{i}.* {label}\n"

                opcoes_validas[str(i)] = label 
                
            texto += "\n_(Digite o n√∫mero da op√ß√£o)_"
        elif dados["tipo"] == "acao_livre":
            texto = "Entendido! Pode digitar sua d√∫vida livremente abaixo: üëá"
            opcoes_validas = "LIVRE"
    return texto, opcoes_validas

def gerar_audio_resposta(texto: str) -> Optional[str]:
    try:
        if not texto: return None
        texto_limpo = texto.replace("*", "").replace("#", "")
        tts = gTTS(text=texto_limpo, lang='pt', tld='com.br')
        pasta_destino = os.path.abspath("assets/audios")
        if not os.path.exists(pasta_destino): os.makedirs(pasta_destino)
        nome_arquivo = f"nubia_{uuid.uuid4()}.mp3"
        caminho_completo = os.path.join(pasta_destino, nome_arquivo)
        tts.save(caminho_completo)
        return caminho_completo
    except Exception as e:
        print(f"‚ö†Ô∏è Erro √°udio: {e}")
        return None

# ---------------------
# Embeddings & Vetoriza√ß√£o - (SentenceTransformer n√£o depende da OpenAI)
# ---------------------
def get_modelo_sentenca():
    global modelo_sentenca
    if modelo_sentenca is None:
        print("üîπ Carregando modelo de embeddings (SentenceTransformer)...")
        modelo_sentenca = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
    return modelo_sentenca

def conectar_sheets(aba: str):
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
    creds = Credentials.from_service_account_file(NUBIA_CREDENTIALS, scopes=SCOPES)
    client = gspread.authorize(creds)
    return client.open(MASTER_SPREADSHEET_NAME).worksheet(aba)

def carregar_base_conhecimento():
    return conectar_sheets("perguntas").get_all_records()

def vetorizar_base_conhecimento(force_reload: bool = False) -> Tuple[dict, list]:
    """
    Vetoriza a base e salva em cache.
    BLINDAGEM: Remove espa√ßos em branco dos t√≥picos para garantir match exato com o menu.
    """
    if os.path.exists(CACHE_VETORES) and not force_reload:
        try:
            print("üíæ Tentando carregar cache de vetores...")
            with open(CACHE_VETORES, "rb") as f:
                dados = pickle.load(f)
                if isinstance(dados, tuple) and len(dados) == 2:
                    return dados[0], dados[1]
                else:
                    print("‚ö†Ô∏è Cache inv√°lido. Recalculando...")
        except Exception as e:
            print(f"‚ö†Ô∏è Falha ao ler cache ({e}). Recalculando...")

    print("üß† Recalculando vetores (Limpando sujeira dos dados)...")
    modelo_ia = get_modelo_sentenca()
    base = carregar_base_conhecimento()
    cerebro = {}

    topicos_sujos = set(l.get("topico", "Outros Assuntos") for l in base)
    topicos_limpos = sorted(list(set([str(t).strip() for t in topicos_sujos if t])))
    
    if "Outros Assuntos" not in topicos_limpos:
        topicos_limpos.append("Outros Assuntos")

    print(f"üìã T√≥picos encontrados na Planilha: {topicos_limpos}")

    for topico in topicos_limpos:

        linhas = [
            l for l in base 
            if str(l.get("topico", "Outros Assuntos")).strip() == topico
        ]
        
        if not linhas:
            continue
            
        docs = [f"{l['Pergunta_Chave']} " * 5 + f"{l['Resposta_Crua']}" for l in linhas]
        vetores = modelo_ia.encode(docs, convert_to_tensor=True)
        
        cerebro[topico] = {"vetores": vetores, "linhas": linhas}

    try:
        with open(CACHE_VETORES, "wb") as f:
            pickle.dump((cerebro, topicos_limpos), f)
            print("üíæ Novo cache limpo e salvo!")
    except Exception as e:
        print(f"[WARN] Erro ao salvar cache: {e}")

    return cerebro, topicos_limpos

# ---------------------
# Busca (vetorial)
# ---------------------
def encontrar_resposta_correspondente(pergunta: str, topico_sugerido: str, cerebro: dict) -> Optional[dict]:
    modelo = get_modelo_sentenca()

    def buscar_em_um_topico(nome_topico: str):
        nome_limpo = nome_topico.strip()
        if nome_limpo not in cerebro: return None, 0.0
            
        dados = cerebro[nome_limpo]
        vetor_usuario = modelo.encode([pergunta], convert_to_tensor=True)
        similaridades = util.pytorch_cos_sim(vetor_usuario, dados["vetores"])[0]
        
        siglas = ["SERCRE", "SESAI", "SEABE", "SERSAO", "SERAMO", "NUBES", "NUTRI√á√ÉO", "ODONTO", "ATESTADO", "HOMOLOGAR"]
        p_upper = pergunta.upper()
        for i, linha in enumerate(dados["linhas"]):
            conteudo = (str(linha.get('Pergunta_Chave','')) + " " + str(linha.get('Resposta_Crua',''))).upper()
            for s in siglas:
                if s in p_upper and s in conteudo: similaridades[i] += 0.25
                    
        melhor_score = torch.max(similaridades).item()
        idx = torch.argmax(similaridades).item()
        return dados["linhas"][idx], melhor_score

    print(f"üîç Buscando em: '{topico_sugerido}'")
    resultado, score = buscar_em_um_topico(topico_sugerido)
    
    if score >= 0.65: 
        print(f"üéØ Alvo Forte encontrado! Score: {score:.3f}")
        resultado["_score"] = score
        return resultado
    

    if score >= 0.35:
        print(f"‚ö†Ô∏è Alvo M√©dio encontrado. Score: {score:.3f}")
        resultado["_score"] = score
        return resultado

    print(f"‚ö†Ô∏è Nada bom em '{topico_sugerido}' (Score: {score:.3f}). Tentando vizinhos...")
    melhor_resultado_global = None
    melhor_score_global = 0.0
    
    for topico_atual in cerebro.keys():
        if topico_atual == topico_sugerido: continue
        res_temp, score_temp = buscar_em_um_topico(topico_atual)
        if score_temp > melhor_score_global:
            melhor_score_global = score_temp
            melhor_resultado_global = res_temp

    if melhor_score_global >= 0.40:
        print(f"üåç Achado em '{melhor_resultado_global.get('topico')}'. Score: {melhor_score_global:.3f}")
        melhor_resultado_global["_score"] = melhor_score_global
        return melhor_resultado_global
        
    return None

# ---------------------
# FUN√á√ïES DE LLM (HUMANIZA√á√ÉO, PRIVACIDADE, CLASSIFICA√á√ÉO DE T√ìPICO, VERIFICAR RESPOSTA E EXPLICA√á√ÉO DA RESPOSTA)
# ---------------------
def humanizar_resposta_com_ia(dado: dict, pergunta_usuario: str) -> str:
    resposta_crua = dado.get("Resposta_Crua", "")
    setor = dado.get("Setor_Responsavel", "")
    base_legal = dado.get("base_legal", "")
    

    texto_setor = ""
    if setor and setor not in ["NUBES", "Setor Respons√°vel", ""]:
        texto_setor = f"\n\nPara mais orienta√ß√µes, a equipe do *{setor}* est√° √† disposi√ß√£o."

    print(f"\nüìù [HUMANIZER INPUT] Base de Dados entregou: '{resposta_crua}'")

    prompt = f"""
Atue como um Formatador de Texto Estrito.
Sua miss√£o √© reescrever a "RESPOSTA T√âCNICA" abaixo para torn√°-la amig√°vel ao usu√°rio.

‚ö†Ô∏è REGRAS DE OURO (Siga rigorosamente):
1. USE APENAS AS INFORMA√á√ïES DA "RESPOSTA T√âCNICA".
2. N√ÉO adicione procedimentos externos (como "procure o RH") se n√£o estiver escrito no texto.
3. N√ÉO invente passos que n√£o existam na fonte.
4. Se a resposta t√©cnica disser "N√£o √© necess√°rio", MANTENHA essa informa√ß√£o.

DADOS:
- Pergunta do Usu√°rio: "{pergunta_usuario}"
- RESPOSTA T√âCNICA (Sua √öNICA fonte de verdade): "{resposta_crua}"
- Base Legal: "{base_legal}"

Gere a resposta final amig√°vel agora:
"""
    try:
        resp = consultar_openai(HUMANIZE_MODEL, prompt, system_msg="Voc√™ √© um redator que obedece estritamente a fonte de dados.")
        
        return (resp + texto_setor) if resp else (resposta_crua + texto_setor)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na humaniza√ß√£o: {e}")
        return resposta_crua + texto_setor

def verificar_privacidade(pergunta: str) -> str:
    prompt = f"""
Classifique como INSEGURO se a pergunta pedir dados pessoais de terceiros ou for ofensiva.
Classifique como SEGURO se for d√∫vida sobre regras, leis ou dados do pr√≥prio usu√°rio.
Pergunta: "{pergunta}"
Responda APENAS: SEGURO ou INSEGURO.
"""
    resp = consultar_openai(PRIVACY_MODEL, prompt, max_tokens=10)
    if resp and "INSEGURO" in resp.upper(): return "INSEGURO"
    return "SEGURO"

def classificar_topico_inteligente(pergunta: str, lista_todos_topicos: list) -> str:
    """
    Recebe a pergunta e a lista de TODOS os t√≥picos do sistema.
    O GPT decide qual √© o melhor encaixe, ignorando onde o usu√°rio clicou.
    """
    lista_formatada = "\n".join([f"- {t}" for t in lista_todos_topicos if t != "Outros Assuntos"])
    
    prompt = f"""
Voc√™ √© um triador especialista. O usu√°rio fez uma pergunta, mas pode estar no menu errado.
Analise a pergunta e diga em qual desses T√≥picos ela se encaixa melhor.

LISTA DE T√ìPICOS V√ÅLIDOS:
{lista_formatada}
- Outros Assuntos

PERGUNTA DO USU√ÅRIO: "{pergunta}"

REGRA: Retorne APENAS o nome exato do t√≥pico da lista acima. Sem explica√ß√µes.
"""
    resp = consultar_openai("gpt-4o-mini", prompt, max_tokens=60, temperature=0.0)
    
    if not resp:
        return "Outros Assuntos"
        
    topico_sugerido = resp.strip().strip(".").strip('"')

    for t in lista_todos_topicos:
        if t.lower() == topico_sugerido.lower():
            return t
        if t.lower() in topico_sugerido.lower() and len(topico_sugerido) > 5:
            return t
            
    return "Outros Assuntos"

def verificar_resposta_sim_nao(pergunta: str, resposta: str) -> Optional[bool]:
    """
    Verifica se a resposta √© pertinente.
    BLINDAGEM V2: Impede aprova√ß√£o de respostas desconexas mesmo que tenham redirecionamento.
    """
    print(f"\nüßê --- AUDITORIA IA ---")
    print(f"‚ùì Pergunta: {pergunta}")
    print(f"üó£Ô∏è Resposta Candidata: {resposta}")
    
    prompt = f"""
Atue como um analista de suporte s√™nior e c√©tico.
Analise se a RESPOSTA abaixo serve de fato para a PERGUNTA do usu√°rio.

PERGUNTA: "{pergunta}"
RESPOSTA: "{resposta}"

REGRAS DE JULGAMENTO:
1. Se a resposta falar sobre um ASSUNTO DIFERENTE da pergunta, o Veredito DEVE ser N√ÉO (mesmo que indique um setor).
   Exemplo de ERRO: Pergunta "Como autorizo?" vs Resposta "Isso n√£o gera licen√ßa m√©dica". -> VEREDITO: N√ÉO.

2. O redirecionamento para um setor (email/telefone) S√ì √â V√ÅLIDO se o texto explicar que aquele assunto ESPEC√çFICO exige contato humano.

3. Negativas ("n√£o precisa", "n√£o gera") s√≥ s√£o v√°lidas se responderem diretamente ao tema perguntado.

Responda no formato:
RACIOCINIO: [Breve explica√ß√£o cr√≠tica]
VEREDITO: [SIM ou N√ÉO]
"""
    
    try:
        resp = consultar_openai(VERIFY_MODEL, prompt, max_tokens=100, temperature=0.0)
        
        if not resp: return None
            
        print(f"ü§ñ An√°lise do Modelo:\n{resp}\n----------------------")

        resp_upper = resp.upper()
        if "VEREDITO: SIM" in resp_upper: return True
        if "VEREDITO: N√ÉO" in resp_upper or "VEREDITO: NAO" in resp_upper: return False

        if "SIM" in resp_upper: return True
        return False

    except Exception as e:
        print(f"‚ö†Ô∏è Erro no validador: {e}")
        return None

def expandir_resposta_com_ia(dado: dict, pergunta_usuario: str) -> str:
    prompt = f"Explique melhor: Pergunta: {pergunta_usuario} | Info: {dado.get('Resposta_Crua','')}"
    resp = consultar_openai(EXPAND_MODEL, prompt)
    return resp if resp else dado.get("Resposta_Crua", "")

# ---------------------
# SEGURAN√áA: SANITIZA√á√ÉO (REGEX)
# ---------------------
def mascarar_dados_sensiveis(texto: str) -> str:
    """
    Remove padr√µes de CPF, Matr√≠cula e E-mails pessoais antes de enviar ao LLM.
    Defesa em profundidade.
    """
    texto_seguro = texto

    # 1. Mascarar CPF (com ou sem pontua√ß√£o)
    padrao_cpf = r'\b\d{3}\.?\d{3}\.?\d{3}-?\d{2}\b'
    texto_seguro = re.sub(padrao_cpf, "[CPF_REMOVIDO]", texto_seguro)

    # 2. Mascarar Matr√≠cula
    padrao_matricula = r'\b\d{5,9}\b'
    texto_seguro = re.sub(padrao_matricula, "[MATRICULA_REMOVIDA]", texto_seguro)
    
    # 3. Mascarar E-mail
    padrao_email = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    texto_seguro = re.sub(padrao_email, "[EMAIL_REMOVIDO]", texto_seguro)

    return texto_seguro

# ---------------------
# LOGS 
# ---------------------
def logar_pergunta_nao_respondida(pergunta: str, nome_usuario: str):
    try:
        aba = conectar_sheets("nao_respondida")
        aba.append_row([str(datetime.now()), nome_usuario, pergunta, "N√ÉO RESPONDIDA"])
    except: pass

def consultar_gemini(prompt: str, sistema: Optional[str] = None, modelo: str = "gpt-4o") -> str:
    """
    Fun√ß√£o de compatibilidade para o nubia_core.py n√£o quebrar.
    Redireciona para consultar_openai.
    """
    sys_msg = sistema if sistema else "Voc√™ √© um assistente √∫til."
    resp = consultar_openai(model=modelo, prompt=prompt, system_msg=sys_msg)
    return resp if resp else ""

def logar_nps(nota: int, comentario: str, telefone: str):
    """
    Salva a nota de satisfa√ß√£o (1-5) no Google Sheets.
    """
    print(f"[METRICA] ‚≠ê NPS Recebido: {nota} - Cliente: {telefone}")
    try:
        aba = conectar_sheets("nps") 
        aba.append_row([str(datetime.now()), telefone, nota, comentario])
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao salvar NPS no Sheets: {e}")